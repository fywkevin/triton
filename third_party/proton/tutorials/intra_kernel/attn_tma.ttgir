#blocked = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [8], order = [0]}>
#blocked1 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [2, 4], order = [1, 0]}>
#loc = loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":104:0)
#loc1 = loc(unknown)
#loc24 = loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":158:74)
#loc38 = loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":69:46)
#loc45 = loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":72:25)
#mma = #triton_gpu.nvidia_mma<{versionMajor = 3, versionMinor = 0, warpsPerCTA = [8, 1], instrShape = [16, 128, 16]}>
#shared = #triton_gpu.shared<{vec = 8, perPhase = 1, maxPhase = 8, order = [1, 0], hasLeadingOffset = true}>
#shared1 = #triton_gpu.shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [0], hasLeadingOffset = false}>
#shared2 = #triton_gpu.shared<{vec = 8, perPhase = 1, maxPhase = 8, order = [0, 1], hasLeadingOffset = true}>
#loc83 = loc(callsite(#loc1 at #loc38))
#loc90 = loc(callsite(#loc1 at #loc45))
#loc101 = loc(callsite(#loc83 at #loc24))
#loc104 = loc(callsite(#loc90 at #loc24))
module attributes {"proton.slots" = 256 : i32, "triton_gpu.num-ctas" = 1 : i32, "triton_gpu.num-warps" = 8 : i32, triton_gpu.target = "cuda:90", "triton_gpu.threads-per-warp" = 32 : i32} {
  tt.func public @_attn_fwd_tma(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32} loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":104:0), %arg1: !tt.ptr<bf16> {tt.divisibility = 16 : i32} loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":104:0), %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32} loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":104:0), %arg3: !tt.ptr<i8> {tt.divisibility = 16 : i32} loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":104:0), %arg4: !tt.ptr<i8> {tt.divisibility = 16 : i32} loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":104:0), %arg5: !tt.ptr<i8> {tt.divisibility = 16 : i32} loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":104:0), %arg6: f32 loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":104:0), %arg7: !tt.ptr<f32> {tt.divisibility = 16 : i32} loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":104:0), %arg8: !tt.ptr<i8> {tt.divisibility = 16 : i32} loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":104:0), %arg9: i32 {tt.divisibility = 16 : i32} loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":104:0), %arg10: i32 {tt.divisibility = 16 : i32} loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":104:0), %arg11: i32 {tt.divisibility = 16 : i32} loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":104:0), %arg12: i32 {tt.divisibility = 16 : i32} loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":104:0), %arg13: i32 {tt.divisibility = 16 : i32} loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":104:0), %arg14: i32 {tt.divisibility = 16 : i32} loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":104:0), %arg15: i32 {tt.divisibility = 16 : i32} loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":104:0), %arg16: i32 {tt.divisibility = 16 : i32} loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":104:0), %arg17: i32 {tt.divisibility = 16 : i32} loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":104:0), %arg18: i32 {tt.divisibility = 16 : i32} loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":104:0), %arg19: i32 {tt.divisibility = 16 : i32} loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":104:0), %arg20: i32 {tt.divisibility = 16 : i32} loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":104:0), %arg21: i32 loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":104:0), %arg22: i32 {tt.divisibility = 16 : i32} loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":104:0), %arg23: i32 {tt.divisibility = 16 : i32} loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":104:0), %arg121: !tt.ptr<i32> {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %true = arith.constant true loc(#loc1)
    %c256_i32 = arith.constant 256 : i32 loc(#loc1)
    %c128_i64 = arith.constant 128 : i64 loc(#loc1)
    %c2_i32 = arith.constant 2 : i32 loc(#loc1)
    %c3_i32 = arith.constant 3 : i32 loc(#loc1)
    %c1_i32 = arith.constant 1 : i32 loc(#loc1)
    %c-1_i32 = arith.constant -1 : i32 loc(#loc1)
    %false = arith.constant false loc(#loc1)
    %c128_i32 = arith.constant 128 : i32 loc(#loc1)
    %cst = arith.constant 1.44269502 : f32 loc(#loc1)
    %c0_i32 = arith.constant 0 : i32 loc(#loc1)
    %cst_0 = arith.constant dense<1.000000e+00> : tensor<128xf32, #triton_gpu.slice<{dim = 1, parent = #mma}>> loc(#loc1)
    %cst_1 = arith.constant dense<0xFF800000> : tensor<128xf32, #triton_gpu.slice<{dim = 1, parent = #mma}>> loc(#loc1)
    %cst_2 = arith.constant dense<0.000000e+00> : tensor<128x128xf32, #mma> loc(#loc1)
    %0 = tt.elementwise_inline_asm "fence.proxy.tensormap::generic.acquire.gpu [$1], 128; // $0 dummy reg" {constraints = "=r, l", packed_element = 1 : i32, pure = false} %arg3 : !tt.ptr<i8> -> i32 loc(#loc2)
    %1 = tt.elementwise_inline_asm "fence.proxy.tensormap::generic.acquire.gpu [$1], 128; // $0 dummy reg" {constraints = "=r, l", packed_element = 1 : i32, pure = false} %arg4 : !tt.ptr<i8> -> i32 loc(#loc3)
    %2 = tt.elementwise_inline_asm "fence.proxy.tensormap::generic.acquire.gpu [$1], 128; // $0 dummy reg" {constraints = "=r, l", packed_element = 1 : i32, pure = false} %arg5 : !tt.ptr<i8> -> i32 loc(#loc4)
    %3 = tt.elementwise_inline_asm "fence.proxy.tensormap::generic.acquire.gpu [$1], 128; // $0 dummy reg" {constraints = "=r, l", packed_element = 1 : i32, pure = false} %arg8 : !tt.ptr<i8> -> i32 loc(#loc5)
    %4 = tt.get_program_id x : i32 loc(#loc6)
    %5 = tt.get_program_id y : i32 loc(#loc7)
    %6 = arith.divsi %5, %arg22 : i32 loc(#loc8)
    %7 = arith.remsi %5, %arg22 : i32 loc(#loc9)
    %8 = arith.extsi %6 : i32 to i64 loc(#loc10)
    %9 = arith.extsi %arg9 : i32 to i64 loc(#loc11)
    %10 = arith.muli %8, %9 : i64 loc(#loc11)
    %11 = arith.extsi %7 : i32 to i64 loc(#loc12)
    %12 = arith.extsi %arg10 : i32 to i64 loc(#loc13)
    %13 = arith.muli %11, %12 : i64 loc(#loc13)
    %14 = arith.addi %10, %13 : i64 loc(#loc14)
    %15 = arith.muli %4, %c128_i32 : i32 loc(#loc15)
    %16 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #blocked> loc(#loc16)
    %17 = tt.splat %15 : i32 -> tensor<128xi32, #blocked> loc(#loc17)
    %18 = arith.addi %17, %16 : tensor<128xi32, #blocked> loc(#loc17)
    %19 = arith.mulf %arg6, %cst : f32 loc(#loc18)
    %20 = arith.extsi %arg11 : i32 to i64 loc(#loc19)
    %21 = arith.divsi %14, %20 : i64 loc(#loc19)
    %22 = arith.extsi %15 : i32 to i64 loc(#loc20)
    %23 = arith.addi %21, %22 : i64 loc(#loc20)
    %24 = arith.trunci %23 : i64 to i32 loc(#loc21)
    %25 = triton_gpu.local_alloc  : () -> !tt.memdesc<128x128xbf16, #shared, #triton_gpu.shared_memory, mutable> loc(#loc22)
    %26 = triton_gpu.local_alloc  : () -> !tt.memdesc<1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc22)
    triton_nvidia_gpu.init_barrier %26, 1 : <1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc22)
    triton_nvidia_gpu.barrier_expect %26, 32768, %true : <1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc22)
    triton_nvidia_gpu.async_tma_copy_global_to_local %arg3[%24, %c0_i32] %25, %26, %true : <i8>, <1xi64, #shared1, #triton_gpu.shared_memory, mutable> -> <128x128xbf16, #shared, #triton_gpu.shared_memory, mutable> loc(#loc22)
    triton_nvidia_gpu.wait_barrier %26, %c0_i32 : <1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc22)
    triton_nvidia_gpu.inval_barrier %26 : <1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc22)
    %27 = triton_gpu.local_load %25 : !tt.memdesc<128x128xbf16, #shared, #triton_gpu.shared_memory, mutable> -> tensor<128x128xbf16, #blocked1> loc(#loc22)
    %28 = triton_gpu.local_alloc %27 : (tensor<128x128xbf16, #blocked1>) -> !tt.memdesc<128x128xbf16, #shared, #triton_gpu.shared_memory> loc(#loc22)
    %29 = arith.extsi %arg14 : i32 to i64 loc(#loc69)
    %30 = tt.splat %19 : f32 -> tensor<128xf32, #triton_gpu.slice<{dim = 1, parent = #mma}>> loc(#loc70)
    %31 = tt.splat %19 : f32 -> tensor<128x128xf32, #mma> loc(#loc71)
    %32 = arith.extsi %arg17 : i32 to i64 loc(#loc72)
    %33 = triton_gpu.local_alloc  : () -> !tt.memdesc<3x128x128xbf16, #shared, #triton_gpu.shared_memory, mutable> loc(#loc73)
    %34 = triton_gpu.local_alloc  : () -> !tt.memdesc<3x128x128xbf16, #shared, #triton_gpu.shared_memory, mutable> loc(#loc74)
    %35 = triton_gpu.local_alloc  : () -> !tt.memdesc<3xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc75)
    %36 = triton_gpu.memdesc_subview %35[%c0_i32] : !tt.memdesc<3xi64, #shared1, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc75)
    triton_nvidia_gpu.init_barrier %36, 1 : <1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc75)
    %37 = triton_gpu.memdesc_subview %35[%c1_i32] : !tt.memdesc<3xi64, #shared1, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc75)
    triton_nvidia_gpu.init_barrier %37, 1 : <1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc75)
    %38 = triton_gpu.memdesc_subview %35[%c2_i32] : !tt.memdesc<3xi64, #shared1, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc75)
    triton_nvidia_gpu.init_barrier %38, 1 : <1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc75)
    %39 = triton_gpu.local_alloc  : () -> !tt.memdesc<3xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc75)
    %40 = triton_gpu.memdesc_subview %39[%c0_i32] : !tt.memdesc<3xi64, #shared1, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc75)
    triton_nvidia_gpu.init_barrier %40, 1 : <1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc75)
    %41 = triton_gpu.memdesc_subview %39[%c1_i32] : !tt.memdesc<3xi64, #shared1, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc75)
    triton_nvidia_gpu.init_barrier %41, 1 : <1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc75)
    %42 = triton_gpu.memdesc_subview %39[%c2_i32] : !tt.memdesc<3xi64, #shared1, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc75)
    triton_nvidia_gpu.init_barrier %42, 1 : <1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc75)
    %43 = arith.cmpi sgt, %arg23, %c0_i32 : i32 loc(#loc75)
    %44 = arith.divsi %14, %29 : i64 loc(#loc69)
    %45 = arith.trunci %44 : i64 to i32 loc(#loc76)
    triton_nvidia_gpu.barrier_expect %36, 32768, %43 : <1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc75)
    %46 = triton_gpu.memdesc_subview %33[%c0_i32, %c0_i32, %c0_i32] : !tt.memdesc<3x128x128xbf16, #shared, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<128x128xbf16, #shared, #triton_gpu.shared_memory, mutable> loc(#loc73)
    triton_nvidia_gpu.async_tma_copy_global_to_local %arg4[%45, %c0_i32] %46, %36, %43 : <i8>, <1xi64, #shared1, #triton_gpu.shared_memory, mutable> -> <128x128xbf16, #shared, #triton_gpu.shared_memory, mutable> loc(#loc73)
    %47 = arith.divsi %14, %32 : i64 loc(#loc72)
    %48 = arith.trunci %47 : i64 to i32 loc(#loc77)
    triton_nvidia_gpu.barrier_expect %40, 32768, %43 : <1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc75)
    %49 = triton_gpu.memdesc_subview %34[%c0_i32, %c0_i32, %c0_i32] : !tt.memdesc<3x128x128xbf16, #shared, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<128x128xbf16, #shared, #triton_gpu.shared_memory, mutable> loc(#loc74)
    triton_nvidia_gpu.async_tma_copy_global_to_local %arg5[%48, %c0_i32] %49, %40, %43 : <i8>, <1xi64, #shared1, #triton_gpu.shared_memory, mutable> -> <128x128xbf16, #shared, #triton_gpu.shared_memory, mutable> loc(#loc74)
    %50 = arith.cmpi sgt, %arg23, %c128_i32 : i32 loc(#loc75)
    %51 = arith.addi %45, %c128_i32 : i32 loc(#loc78)
    triton_nvidia_gpu.barrier_expect %37, 32768, %50 : <1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc75)
    %52 = triton_gpu.memdesc_subview %33[%c1_i32, %c0_i32, %c0_i32] : !tt.memdesc<3x128x128xbf16, #shared, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<128x128xbf16, #shared, #triton_gpu.shared_memory, mutable> loc(#loc73)
    triton_nvidia_gpu.async_tma_copy_global_to_local %arg4[%51, %c0_i32] %52, %37, %50 : <i8>, <1xi64, #shared1, #triton_gpu.shared_memory, mutable> -> <128x128xbf16, #shared, #triton_gpu.shared_memory, mutable> loc(#loc73)
    %53 = arith.addi %47, %c128_i64 : i64 loc(#loc79)
    %54 = arith.trunci %53 : i64 to i32 loc(#loc77)
    triton_nvidia_gpu.barrier_expect %41, 32768, %50 : <1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc75)
    %55 = triton_gpu.memdesc_subview %34[%c1_i32, %c0_i32, %c0_i32] : !tt.memdesc<3x128x128xbf16, #shared, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<128x128xbf16, #shared, #triton_gpu.shared_memory, mutable> loc(#loc74)
    triton_nvidia_gpu.async_tma_copy_global_to_local %arg5[%54, %c0_i32] %55, %41, %50 : <i8>, <1xi64, #shared1, #triton_gpu.shared_memory, mutable> -> <128x128xbf16, #shared, #triton_gpu.shared_memory, mutable> loc(#loc74)
    triton_nvidia_gpu.fence_async_shared {bCluster = false} loc(#loc80)
    %56:6 = scf.for %arg24 = %c0_i32 to %arg23 step %c128_i32 iter_args(%arg25 = %cst_0, %arg26 = %cst_2, %arg27 = %cst_1, %arg28 = %c1_i32, %arg29 = %c-1_i32, %arg30 = %c0_i32) -> (tensor<128xf32, #triton_gpu.slice<{dim = 1, parent = #mma}>>, tensor<128x128xf32, #mma>, tensor<128xf32, #triton_gpu.slice<{dim = 1, parent = #mma}>>, i32, i32, i32)  : i32 {
      tt.proton_record <0, "start", "cycle", "warpgroup">
      %75 = arith.subi %arg23, %c256_i32 : i32 loc(#loc75)
      %76 = arith.cmpi slt, %arg24, %75 : i32 loc(#loc75)
      %77 = arith.addi %arg29, %c1_i32 : i32 loc(#loc75)
      %78 = arith.cmpi slt, %77, %c3_i32 : i32 loc(#loc75)
      %79 = arith.select %78, %77, %c0_i32 : i32 loc(#loc75)
      %80 = arith.xori %arg30, %c1_i32 : i32 loc(#loc75)
      %81 = arith.select %78, %arg30, %80 : i32 loc(#loc75)
      %82 = triton_gpu.memdesc_subview %35[%79] : !tt.memdesc<3xi64, #shared1, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc75)
      tt.proton_record <0, "end", "cycle", "warpgroup">
      triton_nvidia_gpu.wait_barrier %82, %81 : <1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc75)
      tt.proton_record <1, "start", "cycle", "warpgroup">
      %83 = triton_gpu.memdesc_subview %33[%79, %c0_i32, %c0_i32] : !tt.memdesc<3x128x128xbf16, #shared, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<128x128xbf16, #shared, #triton_gpu.shared_memory, mutable> loc(#loc73)
      %84 = tt.trans %83 {order = array<i32: 1, 0>} : !tt.memdesc<128x128xbf16, #shared, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<128x128xbf16, #shared2, #triton_gpu.shared_memory, mutable> loc(#loc81)
      %85 = triton_nvidia_gpu.warp_group_dot %28, %84, %cst_2, %false {inputPrecision = 0 : i32, isAsync = true} : !tt.memdesc<128x128xbf16, #shared, #triton_gpu.shared_memory> * !tt.memdesc<128x128xbf16, #shared2, #triton_gpu.shared_memory, mutable> -> tensor<128x128xf32, #mma> loc(#loc80)
      tt.proton_record <1, "end", "cycle", "warpgroup">
      %86:4 = triton_nvidia_gpu.warp_group_dot_wait %85, %28, %84, %arg26 {pendings = 0 : i32} : tensor<128x128xf32, #mma>, !tt.memdesc<128x128xbf16, #shared, #triton_gpu.shared_memory>, !tt.memdesc<128x128xbf16, #shared2, #triton_gpu.shared_memory, mutable>, tensor<128x128xf32, #mma> loc(#loc80)
      tt.proton_record <2, "start", "cycle", "warpgroup">
      %87 = "tt.reduce"(%86#0) <{axis = 1 : i32}> ({
      ^bb0(%arg31: f32 loc(callsite(#loc83 at #loc24)), %arg32: f32 loc(callsite(#loc83 at #loc24))):
        %120 = arith.maxnumf %arg31, %arg32 : f32 loc(#loc106)
        tt.reduce.return %120 : f32 loc(#loc100)
      }) : (tensor<128x128xf32, #mma>) -> tensor<128xf32, #triton_gpu.slice<{dim = 1, parent = #mma}>> loc(#loc100)
      %88 = arith.mulf %87, %30 : tensor<128xf32, #triton_gpu.slice<{dim = 1, parent = #mma}>> loc(#loc70)
      %89 = arith.maxnumf %arg27, %88 : tensor<128xf32, #triton_gpu.slice<{dim = 1, parent = #mma}>> loc(#loc85)
      %90 = arith.mulf %86#0, %31 : tensor<128x128xf32, #mma> loc(#loc71)
      %91 = tt.expand_dims %89 {axis = 1 : i32} : tensor<128xf32, #triton_gpu.slice<{dim = 1, parent = #mma}>> -> tensor<128x1xf32, #mma> loc(#loc86)
      %92 = tt.broadcast %91 : tensor<128x1xf32, #mma> -> tensor<128x128xf32, #mma> loc(#loc87)
      %93 = arith.subf %90, %92 : tensor<128x128xf32, #mma> loc(#loc87)
      %94 = math.exp2 %93 : tensor<128x128xf32, #mma> loc(#loc88)
      %95 = "tt.reduce"(%94) <{axis = 1 : i32}> ({
      ^bb0(%arg31: f32 loc(callsite(#loc90 at #loc24)), %arg32: f32 loc(callsite(#loc90 at #loc24))):
        %120 = arith.addf %arg31, %arg32 : f32 loc(#loc107)
        tt.reduce.return %120 : f32 loc(#loc103)
      }) : (tensor<128x128xf32, #mma>) -> tensor<128xf32, #triton_gpu.slice<{dim = 1, parent = #mma}>> loc(#loc103)
      %96 = arith.subf %arg27, %89 : tensor<128xf32, #triton_gpu.slice<{dim = 1, parent = #mma}>> loc(#loc92)
      %97 = math.exp2 %96 : tensor<128xf32, #triton_gpu.slice<{dim = 1, parent = #mma}>> loc(#loc93)
      %98 = arith.mulf %arg25, %97 : tensor<128xf32, #triton_gpu.slice<{dim = 1, parent = #mma}>> loc(#loc94)
      %99 = arith.addf %98, %95 : tensor<128xf32, #triton_gpu.slice<{dim = 1, parent = #mma}>> loc(#loc95)
      %100 = tt.expand_dims %97 {axis = 1 : i32} : tensor<128xf32, #triton_gpu.slice<{dim = 1, parent = #mma}>> -> tensor<128x1xf32, #mma> loc(#loc96)
      %101 = tt.broadcast %100 : tensor<128x1xf32, #mma> -> tensor<128x128xf32, #mma> loc(#loc97)
      %102 = arith.mulf %86#3, %101 : tensor<128x128xf32, #mma> loc(#loc97)
      %103 = triton_gpu.memdesc_subview %39[%79] : !tt.memdesc<3xi64, #shared1, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc75)
      tt.proton_record <2, "end", "cycle", "warpgroup">
      triton_nvidia_gpu.wait_barrier %103, %81 : <1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc75)
      tt.proton_record <3, "start", "cycle", "warpgroup">
      %104 = triton_gpu.memdesc_subview %34[%79, %c0_i32, %c0_i32] : !tt.memdesc<3x128x128xbf16, #shared, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<128x128xbf16, #shared, #triton_gpu.shared_memory, mutable> loc(#loc74)
      %105 = arith.truncf %94 : tensor<128x128xf32, #mma> to tensor<128x128xbf16, #mma> loc(#loc98)
      %106 = triton_gpu.convert_layout %105 : tensor<128x128xbf16, #mma> -> tensor<128x128xbf16, #triton_gpu.dot_op<{opIdx = 0, parent = #mma}>> loc(#loc99)
      triton_nvidia_gpu.fence_async_shared {bCluster = false} loc(#loc99)
      %107 = triton_nvidia_gpu.warp_group_dot %106, %104, %102 {inputPrecision = 0 : i32, isAsync = true} : tensor<128x128xbf16, #triton_gpu.dot_op<{opIdx = 0, parent = #mma}>> * !tt.memdesc<128x128xbf16, #shared, #triton_gpu.shared_memory, mutable> -> tensor<128x128xf32, #mma> loc(#loc99)
      %108 = arith.addi %arg28, %c1_i32 : i32 loc(#loc75)
      %109 = arith.cmpi slt, %108, %c3_i32 : i32 loc(#loc75)
      %110 = arith.select %109, %108, %c0_i32 : i32 loc(#loc75)
      %111 = arith.addi %arg24, %c256_i32 : i32 loc(#loc75)
      %112 = arith.addi %111, %45 : i32 loc(#loc78)
      %113 = triton_gpu.memdesc_subview %35[%110] : !tt.memdesc<3xi64, #shared1, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc75)
      tt.proton_record <3, "end", "cycle", "warpgroup">
      triton_nvidia_gpu.barrier_expect %113, 32768, %76 : <1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc75)
      tt.proton_record <4, "start", "cycle", "warpgroup">
      %114 = triton_gpu.memdesc_subview %33[%110, %c0_i32, %c0_i32] : !tt.memdesc<3x128x128xbf16, #shared, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<128x128xbf16, #shared, #triton_gpu.shared_memory, mutable> loc(#loc73)
      triton_nvidia_gpu.async_tma_copy_global_to_local %arg4[%112, %c0_i32] %114, %113, %76 : <i8>, <1xi64, #shared1, #triton_gpu.shared_memory, mutable> -> <128x128xbf16, #shared, #triton_gpu.shared_memory, mutable> loc(#loc73)
      %115 = arith.extsi %111 : i32 to i64 loc(#loc79)
      %116 = arith.addi %47, %115 : i64 loc(#loc79)
      %117 = arith.trunci %116 : i64 to i32 loc(#loc77)
      %118 = triton_gpu.memdesc_subview %39[%110] : !tt.memdesc<3xi64, #shared1, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc75)
      tt.proton_record <4, "end", "cycle", "warpgroup">
      triton_nvidia_gpu.barrier_expect %118, 32768, %76 : <1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc75)
      tt.proton_record <5, "start", "cycle", "warpgroup">
      %119 = triton_gpu.memdesc_subview %34[%110, %c0_i32, %c0_i32] : !tt.memdesc<3x128x128xbf16, #shared, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<128x128xbf16, #shared, #triton_gpu.shared_memory, mutable> loc(#loc74)
      triton_nvidia_gpu.async_tma_copy_global_to_local %arg5[%117, %c0_i32] %119, %118, %76 : <i8>, <1xi64, #shared1, #triton_gpu.shared_memory, mutable> -> <128x128xbf16, #shared, #triton_gpu.shared_memory, mutable> loc(#loc74)
      tt.proton_record <5, "end", "cycle", "warpgroup">
      scf.yield %99, %107, %89, %110, %79, %81 : tensor<128xf32, #triton_gpu.slice<{dim = 1, parent = #mma}>>, tensor<128x128xf32, #mma>, tensor<128xf32, #triton_gpu.slice<{dim = 1, parent = #mma}>>, i32, i32, i32 loc(#loc75)
    } loc(#loc75)
    %57 = triton_nvidia_gpu.warp_group_dot_wait %56#1 {pendings = 0 : i32} : tensor<128x128xf32, #mma> loc(#loc75)
    %58 = triton_gpu.async_wait  {num = 0 : i32} loc(#loc75)
    triton_nvidia_gpu.inval_barrier %36 : <1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc75)
    triton_nvidia_gpu.inval_barrier %37 : <1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc75)
    triton_nvidia_gpu.inval_barrier %38 : <1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc75)
    triton_nvidia_gpu.inval_barrier %40 : <1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc75)
    triton_nvidia_gpu.inval_barrier %41 : <1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc75)
    triton_nvidia_gpu.inval_barrier %42 : <1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc75)
    triton_gpu.local_dealloc %33 : !tt.memdesc<3x128x128xbf16, #shared, #triton_gpu.shared_memory, mutable> loc(#loc75)
    triton_gpu.local_dealloc %34 : !tt.memdesc<3x128x128xbf16, #shared, #triton_gpu.shared_memory, mutable> loc(#loc75)
    %59 = math.log2 %56#0 : tensor<128xf32, #triton_gpu.slice<{dim = 1, parent = #mma}>> loc(#loc55)
    %60 = arith.addf %56#2, %59 : tensor<128xf32, #triton_gpu.slice<{dim = 1, parent = #mma}>> loc(#loc56)
    %61 = tt.expand_dims %56#0 {axis = 1 : i32} : tensor<128xf32, #triton_gpu.slice<{dim = 1, parent = #mma}>> -> tensor<128x1xf32, #mma> loc(#loc57)
    %62 = tt.broadcast %61 : tensor<128x1xf32, #mma> -> tensor<128x128xf32, #mma> loc(#loc58)
    %63 = arith.divf %57, %62 : tensor<128x128xf32, #mma> loc(#loc58)
    %64 = arith.muli %5, %arg23 : i32 loc(#loc59)
    %65 = tt.addptr %arg7, %64 : !tt.ptr<f32>, i32 loc(#loc60)
    %66 = tt.splat %65 : !tt.ptr<f32> -> tensor<128x!tt.ptr<f32>, #blocked> loc(#loc61)
    %67 = tt.addptr %66, %18 : tensor<128x!tt.ptr<f32>, #blocked>, tensor<128xi32, #blocked> loc(#loc61)
    %68 = triton_gpu.convert_layout %60 : tensor<128xf32, #triton_gpu.slice<{dim = 1, parent = #mma}>> -> tensor<128xf32, #blocked> loc(#loc62)
    tt.store %67, %68 : tensor<128x!tt.ptr<f32>, #blocked> loc(#loc62)
    %69 = arith.truncf %63 : tensor<128x128xf32, #mma> to tensor<128x128xbf16, #mma> loc(#loc63)
    %70 = arith.extsi %arg20 : i32 to i64 loc(#loc64)
    %71 = arith.divsi %14, %70 : i64 loc(#loc64)
    %72 = arith.addi %71, %22 : i64 loc(#loc65)
    %73 = arith.trunci %72 : i64 to i32 loc(#loc66)
    %74 = triton_gpu.local_alloc %69 : (tensor<128x128xbf16, #mma>) -> !tt.memdesc<128x128xbf16, #shared, #triton_gpu.shared_memory, mutable> loc(#loc67)
    triton_nvidia_gpu.fence_async_shared {bCluster = false} loc(#loc67)
    triton_nvidia_gpu.async_tma_copy_local_to_global %arg8[%73, %c0_i32] %74 : <i8>, <128x128xbf16, #shared, #triton_gpu.shared_memory, mutable> loc(#loc67)
    triton_nvidia_gpu.async_tma_store_wait {pendings = 0 : i32} loc(#loc67)
    tt.return loc(#loc68)
  } loc(#loc)
} loc(#loc)
#loc2 = loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":118:30)
#loc3 = loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":120:30)
#loc4 = loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":122:30)
#loc5 = loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":124:30)
#loc6 = loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":127:28)
#loc7 = loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":128:27)
#loc8 = loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":129:22)
#loc9 = loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":130:21)
#loc10 = loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":131:26)
#loc11 = loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":131:38)
#loc12 = loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":131:59)
#loc13 = loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":131:71)
#loc14 = loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":131:50)
#loc15 = loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":134:23)
#loc16 = loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":134:46)
#loc17 = loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":134:33)
#loc18 = loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":142:16)
#loc19 = loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":146:25)
#loc20 = loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":146:37)
#loc21 = loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":146:59)
#loc22 = loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":148:9)
#loc23 = loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":57:51)
#loc25 = loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":69:51)
#loc26 = loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":70:22)
#loc27 = loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":90:32)
#loc28 = loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":59:12)
#loc29 = loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":92:16)
#loc30 = loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":52:33)
#loc31 = loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":57:65)
#loc32 = loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":90:56)
#loc33 = loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":57:36)
#loc34 = loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":90:44)
#loc35 = loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":62:23)
#loc36 = loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":61:21)
#loc37 = loc("/home/fywkevin/local/triton/python/triton/language/standard.py":184:40)
#loc39 = loc("/home/fywkevin/local/triton/python/triton/language/standard.py":163:27)
#loc40 = loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":69:35)
#loc41 = loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":70:38)
#loc42 = loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":70:33)
#loc43 = loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":71:25)
#loc44 = loc("/home/fywkevin/local/triton/python/triton/language/standard.py":267:36)
#loc46 = loc("/home/fywkevin/local/triton/python/triton/language/standard.py":256:15)
#loc47 = loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":74:35)
#loc48 = loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":74:29)
#loc49 = loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":75:20)
#loc50 = loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":75:28)
#loc51 = loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":77:26)
#loc52 = loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":77:20)
#loc53 = loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":97:21)
#loc54 = loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":98:27)
#loc55 = loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":171:24)
#loc56 = loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":171:11)
#loc57 = loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":172:20)
#loc58 = loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":172:16)
#loc59 = loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":173:26)
#loc60 = loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":173:17)
#loc61 = loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":173:34)
#loc62 = loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":174:21)
#loc63 = loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":175:53)
#loc64 = loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":176:24)
#loc65 = loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":176:36)
#loc66 = loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":176:58)
#loc67 = loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":176:8)
#loc68 = loc("/home/fywkevin/local/exp/profiler/attn_tma_profile.py":175:4)
#loc69 = loc(callsite(#loc23 at #loc24))
#loc70 = loc(callsite(#loc25 at #loc24))
#loc71 = loc(callsite(#loc26 at #loc24))
#loc72 = loc(callsite(#loc27 at #loc24))
#loc73 = loc(callsite(#loc28 at #loc24))
#loc74 = loc(callsite(#loc29 at #loc24))
#loc75 = loc(callsite(#loc30 at #loc24))
#loc76 = loc(callsite(#loc31 at #loc24))
#loc77 = loc(callsite(#loc32 at #loc24))
#loc78 = loc(callsite(#loc33 at #loc24))
#loc79 = loc(callsite(#loc34 at #loc24))
#loc80 = loc(callsite(#loc35 at #loc24))
#loc81 = loc(callsite(#loc36 at #loc24))
#loc82 = loc(callsite(#loc37 at #loc38))
#loc84 = loc(callsite(#loc39 at #loc37))
#loc85 = loc(callsite(#loc40 at #loc24))
#loc86 = loc(callsite(#loc41 at #loc24))
#loc87 = loc(callsite(#loc42 at #loc24))
#loc88 = loc(callsite(#loc43 at #loc24))
#loc89 = loc(callsite(#loc44 at #loc45))
#loc91 = loc(callsite(#loc46 at #loc44))
#loc92 = loc(callsite(#loc47 at #loc24))
#loc93 = loc(callsite(#loc48 at #loc24))
#loc94 = loc(callsite(#loc49 at #loc24))
#loc95 = loc(callsite(#loc50 at #loc24))
#loc96 = loc(callsite(#loc51 at #loc24))
#loc97 = loc(callsite(#loc52 at #loc24))
#loc98 = loc(callsite(#loc53 at #loc24))
#loc99 = loc(callsite(#loc54 at #loc24))
#loc100 = loc(callsite(#loc82 at #loc24))
#loc102 = loc(callsite(#loc84 at #loc38))
#loc103 = loc(callsite(#loc89 at #loc24))
#loc105 = loc(callsite(#loc91 at #loc45))
#loc106 = loc(callsite(#loc102 at #loc24))
#loc107 = loc(callsite(#loc105 at #loc24))
